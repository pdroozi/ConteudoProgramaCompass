{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a440101",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac6d4df0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SparkSession' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m spark = \u001b[43mSparkSession\u001b[49m .builder \\\n\u001b[32m      2\u001b[39m     .master(\u001b[33m\"\u001b[39m\u001b[33mlocal[*]\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m      3\u001b[39m     .appName(\u001b[33m\"\u001b[39m\u001b[33mExercicio Intro2\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m      4\u001b[39m     .getOrCreate()\n\u001b[32m      6\u001b[39m df_nomes = spark.read.csv(\u001b[33m\"\u001b[39m\u001b[33m../Exercicio1/Etapa3/nomes_aleatorios.txt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m df_nomes.show(\u001b[32m5\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'SparkSession' is not defined"
     ]
    }
   ],
   "source": [
    "spark = SparkSession .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Exercicio Intro\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_nomes = spark.read.csv(\"../Exercicio1/Etapa3/nomes_aleatorios.txt\")\n",
    "df_nomes.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f072bd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nomes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "830f0705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|              Nomes|\n",
      "+-------------------+\n",
      "|        Roy Fuhrman|\n",
      "| Roosevelt Kirchner|\n",
      "|    Milton Longoria|\n",
      "|      Thomas Ringel|\n",
      "|      John Mckinney|\n",
      "|Christopher Timothy|\n",
      "|       Lisa Hammond|\n",
      "|       Olga Naranjo|\n",
      "|     Thomas Padgett|\n",
      "|      Samuel Silver|\n",
      "+-------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "df_nomes = df_nomes.withColumnRenamed(\"_c0\", \"Nomes\")\n",
    "df_nomes.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75fdcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_nomes = df_nomes.withColumn(\"Escolaridade\", F.element_at( F.array( F.lit(\"Fundamental\"), F.lit(\"Medio\"), F.lit(\"Superior\")),( F.floor( F.rand() * 3) + 1).cast(\"int\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81af97f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_nomes = df_nomes.withColumn(\"Pais\", F.element_at( F.array( F.lit(\"Brasil\"), F.lit(\"Argentina\"), F.lit(\"Chile\"), F.lit(\"Uruguai\"), F.lit(\"Paraguai\"),\n",
    "                                                              F.lit(\"Bolivia\"), F.lit(\"Peru\"), F.lit(\"Colombia\"), F.lit(\"Venezuela\"), F.lit(\"Equador\"), \n",
    "                                                              F.lit(\"Suriname\"), F.lit(\"Guiana Francesa\"), F.lit(\"Guiana\")),\n",
    "                                                              ( F.floor( F.rand() * 13) + 1).cast(\"int\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab5259d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_nomes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functions \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df_nomes = \u001b[43mdf_nomes\u001b[49m.withColumn(\u001b[33m\"\u001b[39m\u001b[33mAnoNascimento\u001b[39m\u001b[33m\"\u001b[39m, (F.floor(F.rand() * (\u001b[32m2015\u001b[39m - \u001b[32m1945\u001b[39m + \u001b[32m1\u001b[39m)) + \u001b[32m1945\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mint\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'df_nomes' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_nomes = df_nomes.withColumn(\"AnoNascimento\", (F.floor(F.rand() * (2010 - 1945 + 1)) + 1945).cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a870fd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+---------------+-------------+\n",
      "|              Nomes|Escolaridade|           Pais|AnoNascimento|\n",
      "+-------------------+------------+---------------+-------------+\n",
      "|       Lisa Hammond| Fundamental|        Equador|         2006|\n",
      "|     Thomas Padgett|       Medio|        Equador|         2007|\n",
      "|   Matthew Trembley|    Superior|Guiana Francesa|         2005|\n",
      "|       Michael Page|    Superior|         Brasil|         2002|\n",
      "|     Harriet Denham|       Medio|      Argentina|         2005|\n",
      "|Jennifer Vandermoon|       Medio|         Guiana|         2002|\n",
      "|     Antonio Marcum| Fundamental|      Argentina|         2002|\n",
      "|       Fawn Tallent|    Superior|       Colombia|         2001|\n",
      "|       Amanda Perez|       Medio|        Equador|         2001|\n",
      "|      Matthew Kiser| Fundamental|       Colombia|         2002|\n",
      "+-------------------+------------+---------------+-------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "df_select = df_nomes.select(\"Nomes\", \"Escolaridade\", \"Pais\", \"AnoNascimento\").filter(F.col(\"AnoNascimento\") >= 2001)\n",
    "df_select.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d61bb941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+---------------+-------------+\n",
      "|              Nomes|Escolaridade|           Pais|AnoNascimento|\n",
      "+-------------------+------------+---------------+-------------+\n",
      "|       Lisa Hammond| Fundamental|        Equador|         2006|\n",
      "|     Thomas Padgett|       Medio|        Equador|         2007|\n",
      "|   Matthew Trembley|    Superior|Guiana Francesa|         2005|\n",
      "|       Michael Page|    Superior|         Brasil|         2002|\n",
      "|     Harriet Denham|       Medio|      Argentina|         2005|\n",
      "|Jennifer Vandermoon|       Medio|         Guiana|         2002|\n",
      "|     Antonio Marcum| Fundamental|      Argentina|         2002|\n",
      "|       Fawn Tallent|    Superior|       Colombia|         2001|\n",
      "|       Amanda Perez|       Medio|        Equador|         2001|\n",
      "|      Matthew Kiser| Fundamental|       Colombia|         2002|\n",
      "+-------------------+------------+---------------+-------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "df_nomes.createOrReplaceTempView(\"pessoas\")\n",
    "spark.sql(\"SELECT * FROM pessoas WHERE AnoNascimento >= 2001\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6273a17b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2276014"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_nomes.filter((F.col(\"AnoNascimento\") >= 1980) & (F.col(\"AnoNascimento\") <= 1994)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "512fcfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|    qtd|\n",
      "+-------+\n",
      "|2276014|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nomes.createOrReplaceTempView(\"millenials\")\n",
    "spark.sql(\"SELECT COUNT(*) as qtd FROM millenials where AnoNascimento >= 1980 AND AnoNascimento <= 1994\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c19817",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_nomes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_nomes\u001b[49m.createOrReplaceTempView(\u001b[33m\"\u001b[39m\u001b[33mgeracoes\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m resultado = spark.sql(\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33m    select\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[33m        Pais,\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m \u001b[33m    ORDER BY Pais asc, Geracao asc, Quantidade asc\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[33m\"\"\"\u001b[39m)\n\u001b[32m     19\u001b[39m resultado.show(\u001b[32m55\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df_nomes' is not defined"
     ]
    }
   ],
   "source": [
    "df_nomes.createOrReplaceTempView(\"geracoes\")\n",
    "\n",
    "resultado = spark.sql(\"\"\"\n",
    "    select\n",
    "        Pais,\n",
    "        CASE\n",
    "            WHEN AnoNascimento >= 1944 AND AnoNascimento <= 1964 THEN 'Baby Boomers'\n",
    "            WHEN AnoNascimento >= 1965 AND AnoNascimento <= 1979 THEN 'Geração X'\n",
    "            WHEN AnoNascimento >= 1980 AND AnoNascimento <= 1994 THEN 'Millennials'\n",
    "            WHEN AnoNascimento >= 1995 AND AnoNascimento <= 2015 THEN 'Geração Z'\n",
    "        END AS Geracao,\n",
    "        COUNT(*) AS Quantidade\n",
    "    FROM geracoes\n",
    "    WHERE AnoNascimento BETWEEN 1944 AND 2015\n",
    "    GROUP BY Pais, Geracao\n",
    "    ORDER BY Pais asc, Geracao asc, Quantidade asc\n",
    "\"\"\")\n",
    "\n",
    "resultado.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
